# -*- coding: utf-8 -*-
"""Medical_Bias_Checker_Pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12ipaqHMnkkn_Md_YPKA14cmORQGptsHg

# Medical Bias Checker - Few-Shot Prompting Pipeline

## Complete Data Pipeline: Content Input > Bias Detection > Website Deployment

**Project Goal:** Build a few-shot prompting system using the OpenAI (ChatGPT) API that takes medical content and identifies biases across 15 bias types using curated example case studies.

**Tech Stack:** Python + Jupyter Notebook, OpenAI API (GPT-4o), Antigravity (Website), Pandas/JSON

---
### How Few-Shot Prompting Works Here
```
[System Prompt] + [Example 1: User/Assistant] + [Example 2] + ... + [Your Content] --> GPT-4o --> Bias Report (JSON)
```
The model learns from the examples what biased vs unbiased content looks like, then applies that pattern to your content.

## Step 1: Install Dependencies
"""

# STEP 1: Install Dependencies
!pip install openai pandas python-dotenv requests
print("All dependencies installed!")

# STEP 2b: Setup API Key (Securely)
import os
from dotenv import load_dotenv

# Try to load from .env file if available
load_dotenv()

# Get API key from environment variable
api_key = os.environ.get("OPENAI_API_KEY")

if not api_key:
    print("âš ï¸  WARNING: OPENAI_API_KEY not found in environment variables.")
    print("Please set it in your .env file or environment.")
else:
    print(f"âœ… API Key loaded successfully: {api_key[:8]}...")

client = OpenAI(api_key=api_key)

"""## Step 3: Define 15 Bias Types with Medical Context"""

# ============================================================
# STEP 3: Define Bias Categories (Aligned with Your Dataset)
# ============================================================
# Your dataset uses 4 main categories. Each category contains
# specific sub-types of bias that fall under it.

BIAS_CATEGORIES = {

    # ---- CATEGORY 1: NO BIAS ----
    "no_bias": {
        "name": "No Bias",
        "definition": "Content that follows evidence-based, equitable clinical practice with no identifiable bias patterns.",
        "description": "Standard clinical terminology, objective documentation, patient-centered language without demographic assumptions or stigmatizing terms.",
        "characteristics": [
            "Uses neutral, patient-first language",
            "Clinical decisions based on symptoms and evidence, not demographics",
            "Objective documentation of history, exam, and plan",
            "Inclusive and respectful tone"
        ],
        "example_keywords": ["patient", "history", "hypertension", "clinic", "management", "evaluation", "assessment"]
    },

    # ---- CATEGORY 2: DEMOGRAPHIC BIAS ----
    "demographic_bias": {
        "name": "Demographic Bias",
        "definition": "Unfair treatment, assumptions, or clinical decisions driven by a patient's demographic characteristics rather than clinical evidence.",
        "sub_types": {
            "racial_bias": {
                "name": "Racial/Ethnic Bias",
                "definition": "Clinical decisions or assumptions influenced by patient's race or ethnicity.",
                "examples": [
                    "Prioritizing substance abuse screening based on race",
                    "Assuming pain tolerance differs by ethnicity",
                    "Differential treatment recommendations based on racial background"
                ]
            },
            "gender_bias": {
                "name": "Gender Bias",
                "definition": "Differential treatment or dismissal of symptoms based on patient gender.",
                "examples": [
                    "Attributing women's pain to psychological causes without workup",
                    "Assuming pregnancy as primary differential for all reproductive-age women",
                    "Dismissing cardiac symptoms in female patients"
                ]
            },
            "age_bias": {
                "name": "Age Bias",
                "definition": "Assumptions about treatment needs or capabilities based solely on age.",
                "examples": [
                    "Assuming elderly patients have dementia without cognitive testing",
                    "Recommending less aggressive treatment purely due to age",
                    "Dismissing young patient's complaints as trivial"
                ]
            },
            "socioeconomic_bias": {
                "name": "Socioeconomic Bias",
                "definition": "Treatment influenced by perceived socioeconomic status, insurance, or background.",
                "examples": [
                    "Offering fewer treatment options to uninsured patients",
                    "Assuming non-adherence based on income level",
                    "Different referral patterns based on patient's neighborhood"
                ]
            }
        },
        "example_keywords": ["Hispanic", "background", "socioeconomic", "demographic", "population", "community"]
    },

    # ---- CATEGORY 3: CLINICAL STIGMA BIAS ----
    "clinical_stigma_bias": {
        "name": "Clinical Stigma Bias",
        "definition": "Stigmatizing language or assumptions in clinical documentation related to patient conditions, weight, pain, mental health, or lifestyle.",
        "sub_types": {
            "weight_stigma": {
                "name": "Weight/Obesity Stigma",
                "definition": "Bias in language or treatment decisions related to patient weight or body size.",
                "examples": [
                    "Attributing all symptoms to obesity without differential diagnosis",
                    "Using stigmatizing terms like 'morbidly obese' without clinical context",
                    "Withholding treatments until patient loses weight"
                ]
            },
            "pain_stigma": {
                "name": "Pain Dismissal Stigma",
                "definition": "Dismissing, minimizing, or questioning patient pain reports.",
                "examples": [
                    "Labeling patients as 'drug-seeking' without evidence",
                    "Describing pain expression as 'dramatic' or 'exaggerated'",
                    "Undertreating pain based on subjective judgment of patient credibility"
                ]
            },
            "mental_health_stigma": {
                "name": "Mental Health Stigma",
                "definition": "Dismissing physical symptoms by attributing them to mental health conditions.",
                "examples": [
                    "Labeling unexplained symptoms as 'psychosomatic' without workup",
                    "Documenting patient as 'anxious' to dismiss clinical concerns",
                    "Reduced diagnostic effort for patients with psychiatric history"
                ]
            },
            "lifestyle_stigma": {
                "name": "Lifestyle Judgment Stigma",
                "definition": "Moralizing or blaming patients for conditions linked to lifestyle choices.",
                "examples": [
                    "Framing diabetes as consequence of 'poor lifestyle choices'",
                    "Using language like 'non-compliant' or 'refuses to change'",
                    "Attributing addiction to moral failure rather than medical condition"
                ]
            }
        },
        "example_keywords": ["pain", "weight", "obesity", "lifestyle", "choices", "drug-seeking", "non-compliant"]
    },

    # ---- CATEGORY 4: ASSESSMENT BIAS ----
    "assessment_bias": {
        "name": "Assessment Bias",
        "definition": "Bias in clinical evaluation, diagnostic decision-making, or competency assessment driven by assumptions rather than evidence.",
        "sub_types": {
            "diagnostic_bias": {
                "name": "Diagnostic Assessment Bias",
                "definition": "Skewed diagnostic decisions based on non-clinical factors.",
                "examples": [
                    "Anchoring on initial diagnosis despite contradicting evidence",
                    "Ordering fewer tests for certain patient groups",
                    "Premature closure of differential diagnosis"
                ]
            },
            "competency_assessment_bias": {
                "name": "Competency Assessment Bias",
                "definition": "Unfair evaluation of medical trainees or staff based on non-performance factors.",
                "examples": [
                    "Evaluating resident communication style based on cultural norms",
                    "Rating trainees differently based on gender or accent",
                    "Applying different standards for 'professionalism' based on background"
                ]
            },
            "treatment_decision_bias": {
                "name": "Treatment Decision Bias",
                "definition": "Unequal treatment recommendations not justified by clinical differences.",
                "examples": [
                    "Recommending different treatments for identical presentations based on demographics",
                    "Assuming certain patients won't follow through with complex treatment plans",
                    "Offering less aggressive treatment options without clinical justification"
                ]
            }
        },
        "example_keywords": ["communication", "Dr. Smith", "resident", "competence", "evaluation", "assessment"]
    }
}

# ---- Print Summary ----
print(f"Defined {len(BIAS_CATEGORIES)} bias categories:\n")
for key, cat in BIAS_CATEGORIES.items():
    sub_types = cat.get("sub_types", {})
    if sub_types:
        print(f"  {cat['name']}")
        for sk, sv in sub_types.items():
            print(f"    â””â”€â”€ {sv['name']}")
    else:
        print(f"  {cat['name']} (baseline)")
    print()

total_sub = sum(len(cat.get("sub_types", {})) for cat in BIAS_CATEGORIES.values())
print(f"Total: {len(BIAS_CATEGORIES)} categories, {total_sub} sub-types")

"""## Step 4: Create Few-Shot Examples (Medical Case Studies)

These curated examples teach the LLM what each bias looks like. Instead of fine-tuning, we provide examples directly in the prompt.
"""

# STEP 4: Few-Shot Examples - Aligned with Our 4 Categories
# Each example uses: category (no_bias / demographic_bias / clinical_stigma_bias / assessment_bias)
#                     sub_type (the specific bias within that category)

# ---- EXAMPLE 1: DEMOGRAPHIC BIAS (racial + gender sub-types) ----
ex1_result = {
    "bias_detected": True,
    "primary_category": "demographic_bias",
    "overall_bias_level": "HIGH",
    "biases_found": [
        {
            "category": "demographic_bias",
            "sub_type": "racial_bias",
            "confidence": 0.95,
            "evidence": "Prioritizing substance abuse screening based on race rather than clinical presentation demonstrates racial profiling in clinical decision-making.",
            "problematic_text": "considered substance abuse screening as a priority given the patient population",
            "recommendation": "Base screening decisions on clinical indicators (history, symptoms, vital signs) rather than racial demographics."
        },
        {
            "category": "demographic_bias",
            "sub_type": "racial_bias",
            "confidence": 0.78,
            "evidence": "Using 'demographic profile' as basis for clinical decisions rather than individual patient assessment.",
            "problematic_text": "Given his demographic profile",
            "recommendation": "Assess each patient individually using validated clinical screening tools regardless of background."
        }
    ],
    "bias_free_sections": ["standard cardiac workup"],
    "summary": "Demographic bias with racial profiling influencing substance abuse screening decisions instead of clinical evidence."
}

# ---- EXAMPLE 2: CLINICAL STIGMA BIAS (pain + weight + lifestyle sub-types) ----
ex2_result = {
    "bias_detected": True,
    "primary_category": "clinical_stigma_bias",
    "overall_bias_level": "HIGH",
    "biases_found": [
        {
            "category": "clinical_stigma_bias",
            "sub_type": "pain_stigma",
            "confidence": 0.94,
            "evidence": "Patient pain described as 'exaggerated' without validated pain assessment, dismissing reported symptoms.",
            "problematic_text": "patient appeared to be exaggerating her pain levels",
            "recommendation": "Use validated pain assessment tools (NRS, VAS). Document observed behaviors objectively without judgmental language."
        },
        {
            "category": "clinical_stigma_bias",
            "sub_type": "weight_stigma",
            "confidence": 0.91,
            "evidence": "Knee pain attributed entirely to weight without investigating orthopedic, autoimmune, or mechanical causes.",
            "problematic_text": "pain is primarily a consequence of her weight and poor lifestyle choices",
            "recommendation": "Perform complete differential diagnosis. Weight may be a contributing factor but should not preclude investigation of other causes."
        },
        {
            "category": "clinical_stigma_bias",
            "sub_type": "lifestyle_stigma",
            "confidence": 0.88,
            "evidence": "'Poor lifestyle choices' moralizes the patient's condition rather than treating it as a medical issue.",
            "problematic_text": "poor lifestyle choices",
            "recommendation": "Replace judgmental language with neutral clinical terms. Discuss lifestyle factors as modifiable risk factors, not moral failings."
        }
    ],
    "bias_free_sections": [],
    "summary": "Clinical stigma through pain dismissal, weight stigma, and lifestyle judgment. Patient symptoms minimized and attributed to weight/lifestyle rather than properly evaluated."
}

# ---- EXAMPLE 3: NO BIAS (clean documentation) ----
ex3_result = {
    "bias_detected": False,
    "primary_category": "no_bias",
    "overall_bias_level": "NONE",
    "biases_found": [],
    "bias_free_sections": [
        "Comprehensive evaluation with multiple diagnostic tests ordered",
        "Broad differential diagnosis without demographic assumptions",
        "Collaborative treatment planning with patient preferences considered",
        "Follow-up scheduled regardless of initial results",
        "Patient described by age and symptoms only, no stigmatizing language"
    ],
    "summary": "Unbiased clinical documentation demonstrating evidence-based practice, comprehensive workup, collaborative care, and patient-centered language."
}

# ---- EXAMPLE 4: ASSESSMENT BIAS (diagnostic + competency + treatment sub-types) ----
ex4_result = {
    "bias_detected": True,
    "primary_category": "assessment_bias",
    "overall_bias_level": "HIGH",
    "biases_found": [
        {
            "category": "assessment_bias",
            "sub_type": "diagnostic_bias",
            "confidence": 0.92,
            "evidence": "New contradicting evidence (elevated inflammatory markers, positive ANA) was dismissed to maintain original diagnosis, demonstrating diagnostic anchoring and premature closure.",
            "problematic_text": "Despite subsequent lab results showing elevated inflammatory markers and positive ANA, the team continued with the fibromyalgia treatment plan",
            "recommendation": "All new clinical findings should trigger reassessment. Positive ANA and elevated inflammatory markers warrant autoimmune workup."
        },
        {
            "category": "assessment_bias",
            "sub_type": "competency_assessment_bias",
            "confidence": 0.85,
            "evidence": "Junior team members' clinical judgment overridden by seniority rather than evidence, reflecting hierarchical assessment bias.",
            "problematic_text": "the team continued as Dr. Harrison felt confident in the original assessment",
            "recommendation": "Foster a culture where clinical evidence takes priority over hierarchy. Implement structured case review that encourages dissent."
        },
        {
            "category": "assessment_bias",
            "sub_type": "treatment_decision_bias",
            "confidence": 0.80,
            "evidence": "Treatment plan not adjusted despite contradictory lab results, showing biased treatment decision-making anchored to initial impression.",
            "problematic_text": "continued with the fibromyalgia treatment plan",
            "recommendation": "Treatment plans should be living documents updated as new clinical data becomes available."
        }
    ],
    "bias_free_sections": [],
    "summary": "Assessment bias through diagnostic anchoring, hierarchical override of evidence, and failure to update treatment despite contradictory lab results."
}

# ---- EXAMPLE 5: INTERSECTIONAL (demographic + assessment + clinical stigma) ----
ex5_result = {
    "bias_detected": True,
    "primary_category": "demographic_bias",
    "overall_bias_level": "HIGH",
    "biases_found": [
        {
            "category": "demographic_bias",
            "sub_type": "age_bias",
            "confidence": 0.91,
            "evidence": "Assuming dementia based solely on age and brief observation without any standardized cognitive testing.",
            "problematic_text": "Given her age and mental state, we assumed early dementia",
            "recommendation": "Perform standardized cognitive assessment (MMSE, MoCA). Consider delirium, medication effects, and other reversible causes before diagnosing dementia."
        },
        {
            "category": "assessment_bias",
            "sub_type": "diagnostic_bias",
            "confidence": 0.89,
            "evidence": "Premature diagnostic closure without conducting cognitive testing or considering differential diagnoses for acute confusion.",
            "problematic_text": "No further cognitive testing was deemed necessary",
            "recommendation": "Differential should include delirium, UTI, medication side effects, depression, metabolic causes. Always perform formal cognitive assessment."
        },
        {
            "category": "clinical_stigma_bias",
            "sub_type": "mental_health_stigma",
            "confidence": 0.78,
            "evidence": "Labeling confusion and combativeness as dementia stigmatizes the patient's mental state without investigating underlying medical causes.",
            "problematic_text": "confused and combative during examination",
            "recommendation": "Document behaviors objectively. Investigate medical causes of acute confusion before attributing to chronic neurodegenerative conditions."
        }
    ],
    "bias_free_sections": [],
    "summary": "Intersectional bias combining age-based demographic assumptions, premature diagnostic closure (assessment bias), and mental health stigma. Patient needs comprehensive evaluation."
}

# ---- ASSEMBLE FEW-SHOT EXAMPLES ----
FEW_SHOT_EXAMPLES = [
    # Example 1: Demographic Bias
    {"role": "user", "content": "Analyze this medical content for bias:\n\nThe 45-year-old African American male presented with chest pain. Given his demographic profile, we proceeded with a standard cardiac workup but considered substance abuse screening as a priority given the patient population."},
    {"role": "assistant", "content": json.dumps(ex1_result)},

    # Example 2: Clinical Stigma Bias
    {"role": "user", "content": "Analyze this medical content for bias:\n\nThe patient with BMI of 38 presented with chronic knee pain. The patient appeared to be exaggerating her pain levels. The physician documented that her pain is primarily a consequence of her weight and poor lifestyle choices. No imaging was ordered."},
    {"role": "assistant", "content": json.dumps(ex2_result)},

    # Example 3: No Bias
    {"role": "user", "content": "Analyze this medical content for bias:\n\nThe patient, a 58-year-old individual, presented with persistent cough lasting 3 weeks. A comprehensive evaluation was conducted including chest X-ray, CBC, and pulmonary function tests. Differential diagnosis included COPD exacerbation, post-infectious cough, and potential malignancy. Treatment plan developed collaboratively with patient. Follow-up scheduled regardless of initial results."},
    {"role": "assistant", "content": json.dumps(ex3_result)},

    # Example 4: Assessment Bias
    {"role": "user", "content": "Analyze this medical content for bias:\n\nDr. Harrison, the department chief, diagnosed the patient with fibromyalgia based on the initial presentation. Despite subsequent lab results showing elevated inflammatory markers and positive ANA, the team continued with the fibromyalgia treatment plan as Dr. Harrison felt confident in the original assessment."},
    {"role": "assistant", "content": json.dumps(ex4_result)},

    # Example 5: Intersectional (Demographic + Assessment + Clinical Stigma)
    {"role": "user", "content": "Analyze this medical content for bias:\n\nThe elderly patient was confused and combative during examination. Given her age and mental state, we assumed early dementia and recommended nursing home placement. No further cognitive testing was deemed necessary."},
    {"role": "assistant", "content": json.dumps(ex5_result)},
]

print(f"Created {len(FEW_SHOT_EXAMPLES) // 2} few-shot example pairs:")
print("  1: Demographic Bias (racial_bias sub-type)")
print("  2: Clinical Stigma Bias (pain_stigma + weight_stigma + lifestyle_stigma)")
print("  3: No Bias (clean documentation)")
print("  4: Assessment Bias (diagnostic_bias + competency_assessment_bias + treatment_decision_bias)")
print("  5: Intersectional (age_bias + diagnostic_bias + mental_health_stigma)")

"""## Step 5: Build the System Prompt"""

# STEP 5: System Prompt - Aligned with 4 Bias Categories & 11 Sub-Types
SYSTEM_PROMPT = '''You are an expert medical bias detection system. Your role is to analyze medical content, clinical documentation, research papers, and healthcare conversations to identify biases that could affect patient care, clinical decisions, or health equity.

## Your Task
When given medical content, you must:
1. Analyze the content for biases across the 4 categories and 11 sub-types defined below
2. Return a structured JSON response with your findings
3. Provide specific evidence from the text for each bias detected
4. Rate confidence levels (0.0 to 1.0) for each identified bias
5. Suggest concrete, actionable recommendations to mitigate each bias
6. Identify sections that are bias-free as examples of good clinical practice

## The 4 Bias Categories and 11 Sub-Types

### 1. NO BIAS (no_bias)
Content that follows evidence-based, equitable clinical practice. Uses neutral patient-first language, makes clinical decisions based on symptoms and evidence (not demographics), documents objectively, and maintains an inclusive and respectful tone.

### 2. DEMOGRAPHIC BIAS (demographic_bias)
Unfair treatment, assumptions, or clinical decisions driven by patient demographics rather than clinical evidence.
Sub-types:
- racial_bias: Clinical decisions or assumptions influenced by patient race or ethnicity (e.g., screening priorities based on race, assumed pain tolerance by ethnicity)
- gender_bias: Differential treatment or dismissal of symptoms based on gender (e.g., attributing women's pain to psychological causes, assuming pregnancy as default differential)
- age_bias: Assumptions about treatment needs or capabilities based solely on age (e.g., assuming dementia in elderly without testing, dismissing young patients' complaints)
- socioeconomic_bias: Treatment influenced by perceived socioeconomic status, insurance, or background (e.g., fewer options for uninsured, assumptions based on neighborhood)

### 3. CLINICAL STIGMA BIAS (clinical_stigma_bias)
Stigmatizing language or assumptions in clinical documentation related to patient conditions, weight, pain, mental health, or lifestyle.
Sub-types:
- weight_stigma: Bias related to patient weight or body size (e.g., attributing all symptoms to obesity, withholding treatment until weight loss)
- pain_stigma: Dismissing, minimizing, or questioning patient pain reports (e.g., labeling as 'drug-seeking', calling pain expression 'dramatic')
- mental_health_stigma: Dismissing physical symptoms by attributing them to mental health (e.g., labeling as 'psychosomatic' without workup, reduced diagnostic effort for psychiatric patients)
- lifestyle_stigma: Moralizing or blaming patients for conditions linked to lifestyle (e.g., 'poor lifestyle choices', 'non-compliant', treating addiction as moral failure)

### 4. ASSESSMENT BIAS (assessment_bias)
Bias in clinical evaluation, diagnostic decision-making, or competency assessment driven by assumptions rather than evidence.
Sub-types:
- diagnostic_bias: Skewed diagnostic decisions based on non-clinical factors (e.g., anchoring on initial diagnosis, ordering fewer tests for certain groups, premature diagnostic closure)
- competency_assessment_bias: Unfair evaluation of medical trainees or staff based on non-performance factors (e.g., evaluating communication style based on cultural norms, different professionalism standards)
- treatment_decision_bias: Unequal treatment recommendations not justified by clinical differences (e.g., less aggressive treatment without justification, assuming patients won't follow through)

## Response Format (STRICT JSON)
Always respond with ONLY valid JSON in this exact format:
{
    "bias_detected": true/false,
    "primary_category": "no_bias" | "demographic_bias" | "clinical_stigma_bias" | "assessment_bias",
    "overall_bias_level": "NONE" | "LOW" | "MODERATE" | "HIGH" | "CRITICAL",
    "biases_found": [
        {
            "category": "demographic_bias" | "clinical_stigma_bias" | "assessment_bias",
            "sub_type": "the specific sub-type key from above",
            "confidence": 0.0-1.0,
            "evidence": "Detailed explanation of why this is biased",
            "problematic_text": "Exact quote from the content",
            "recommendation": "Specific actionable fix"
        }
    ],
    "bias_free_sections": ["List of well-written, unbiased sections"],
    "summary": "Brief overall assessment"
}

## Rules
- Only use the 4 categories and 11 sub-types defined above â€” do not invent new bias types
- Only flag genuine biases with clear evidence â€” avoid false positives
- Confidence below 0.5 should not be included unless there is a clear pattern
- Always explain WHY something is biased, not just THAT it is biased
- Provide constructive, actionable recommendations for each bias found
- Recognize good clinical practice and note bias-free sections
- Content can contain biases from multiple categories (intersectional bias)
- primary_category should reflect the single most dominant bias type present
- If no bias is found, set primary_category to "no_bias" and bias_detected to false
- Be especially vigilant about biases that directly affect patient safety and outcomes
'''

print(f"System prompt configured ({len(SYSTEM_PROMPT)} chars)")
print(f"Categories: no_bias, demographic_bias, clinical_stigma_bias, assessment_bias")
print(f"Sub-types: 11 total across 3 bias categories")

"""## Step 6: Few-Shot Prompting Engine

The core engine constructs: `[System Prompt] + [Few-Shot Examples] + [User Content]` and sends it to OpenAI.
"""

# STEP 6: Few-Shot Prompting Engine (Updated for 4 Categories + Sub-Types)
class MedicalBiasChecker:
    def __init__(self, model="gpt-4.1", api_key=None):
        self.model = model
        self.client = OpenAI(api_key=api_key or os.getenv("OPENAI_API_KEY"))
        self.system_prompt = SYSTEM_PROMPT
        self.few_shot_examples = FEW_SHOT_EXAMPLES
        self.history = []
        print(f"Bias Checker initialized | Model: {self.model} | Examples: {len(self.few_shot_examples)//2}")
        print(f"Categories: no_bias, demographic_bias, clinical_stigma_bias, assessment_bias")

    def _build_messages(self, content):
        """
        Construct the full message array for few-shot prompting.
        Structure: [System Prompt] + [Few-Shot Example Pairs] + [User Content]
        """
        messages = [{"role": "system", "content": self.system_prompt}]
        messages.extend(self.few_shot_examples)
        messages.append({"role": "user", "content": f"Analyze this medical content for bias:\n\n{content}"})
        return messages

    def analyze(self, content, temperature=0.1, verbose=True):
        if verbose:
            print(f"Analyzing ({len(content)} chars) with {self.model}...")

        start = time.time()
        messages = self._build_messages(content)

        try:
            response = self.client.chat.completions.create(
                model=self.model, messages=messages,
                temperature=temperature, max_tokens=2000,
                response_format={"type": "json_object"}
            )
            result = json.loads(response.choices[0].message.content)
            elapsed = time.time() - start

            self.history.append({
                "content": content[:200], "result": result,
                "model": self.model, "time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "seconds": round(elapsed, 2)
            })

            if verbose: self._print_report(result, elapsed)
            return result
        except Exception as e:
            print(f"Error: {e}")
            return None

    def _print_report(self, result, elapsed):
        """Pretty-print the bias analysis report with category > sub_type format."""
        print("=" * 65)
        print("BIAS ANALYSIS REPORT")
        print("=" * 65)

        level = result.get("overall_bias_level", "?")
        primary = result.get("primary_category", "?")
        icons = {"NONE": "ðŸŸ¢", "LOW": "ðŸŸ¡", "MODERATE": "ðŸŸ ", "HIGH": "ðŸ”´", "CRITICAL": "â›”"}

        print(f"  Bias Detected:    {'Yes' if result.get('bias_detected') else 'No'}")
        print(f"  Primary Category: {primary.replace('_', ' ').title()}")
        print(f"  Bias Level:       {icons.get(level, 'â“')} {level}")
        print(f"  Analysis Time:    {elapsed:.1f}s")

        biases = result.get("biases_found", [])
        if biases:
            print(f"\n  Biases Found: {len(biases)}")
            print("-" * 65)
            for i, b in enumerate(biases, 1):
                category = b.get("category", "unknown").replace("_", " ").title()
                sub_type = b.get("sub_type", "unknown").replace("_", " ").title()
                confidence = b.get("confidence", 0)

                print(f"\n  [{i}] {category} > {sub_type}")
                print(f"      Confidence: {confidence:.0%}")
                print(f"      Evidence:   {b.get('evidence', '')[:120]}")
                print(f"      Text:       \"{b.get('problematic_text', '')[:100]}\"")
                print(f"      Fix:        {b.get('recommendation', '')[:120]}")
        else:
            print("\n  âœ… No biases detected - content appears fair and well-documented!")

        clean = result.get("bias_free_sections", [])
        if clean:
            print(f"\n  Bias-Free Sections:")
            for s in clean:
                print(f"    âœ“ {s}")

        print(f"\n  Summary: {result.get('summary', 'N/A')}")
        print("=" * 65)

    def export_results(self, path="bias_results.json"):
        with open(path, "w") as f: json.dump(self.history, f, indent=2)
        print(f"Exported to {path}")

checker = MedicalBiasChecker(model="gpt-4.1")

"""## Step 7: Test with Medical Case Studies"""

# TEST CASE 1: Biased Clinical Note
test_1 = '''The 32-year-old Hispanic female presented to the ER with abdominal pain.
Given the patient's background and age, pregnancy was considered as the primary
differential. The patient appeared dramatic in her pain expression, which is common
in patients of this cultural background. Basic labs were ordered but advanced imaging
was deferred as the presentation was likely psychosomatic or pregnancy-related.'''

print("TEST 1: Biased Clinical Note")
result_1 = checker.analyze(test_1)

# TEST CASE 2: Biased Research Summary
test_2 = '''Our groundbreaking study on cardiovascular disease risk enrolled 200 patients
from our private cardiology practice. All participants were male, aged 50-65, primarily
Caucasian. Results demonstrated that our new statin protocol reduced LDL by 40%.
Dr. Williams, the renowned cardiologist, confirmed these results represent a paradigm
shift. No adverse events were reported during follow-up.'''

print("TEST 2: Biased Research Summary")
result_2 = checker.analyze(test_2)

# TEST CASE 3: Clean Documentation (should detect no bias)
test_3 = '''The patient, a 52-year-old individual, presented with progressive dyspnea
and bilateral lower extremity edema. Comprehensive workup initiated including BNP,
chest X-ray, echocardiogram, and basic metabolic panel. Differential: CHF, PE, CKD
exacerbation. Treatment developed collaboratively with patient considering preferences,
lifestyle, and socioeconomic factors. Follow-up in 1 week with 48hr phone check-in.'''

print("TEST 3: Clean Documentation")
result_3 = checker.analyze(test_3)

"""## Step 8: Batch Processing"""

# STEP 8: Batch Processing
import pandas as pd

def batch_analyze(checker, contents, labels=None):
    rows = []
    for i, c in enumerate(contents):
        label = labels[i] if labels else f"Content_{i+1}"
        r = checker.analyze(c, verbose=False)
        if r:
            biases = r.get("biases_found", [])
            rows.append({
                "Label": label,
                "Bias?": r.get("bias_detected"),
                "Level": r.get("overall_bias_level"),
                "Count": len(biases),
                "Types": ", ".join(b["bias_type"] for b in biases),
                "Summary": r.get("summary","")[:80]
            })
    return pd.DataFrame(rows)

df = batch_analyze(checker, [test_1, test_2, test_3],
                   ["Biased Note", "Biased Research", "Clean Doc"])
print("\nBATCH SUMMARY:")
display(df)

"""## Step 9: Try Your Own Content"""

# STEP 9: Analyze Your Own Content
my_content = '''The elderly patient was confused and combative during examination.
Given her age and mental state, we assumed early dementia and recommended nursing
home placement. No further cognitive testing was deemed necessary.'''

result = checker.analyze(my_content)

"""## Step 10: Export for Website"""

# STEP 10: Export data files
checker.export_results("bias_analysis_results.json")

with open("bias_types_reference.json", "w") as f:
    json.dump(BIAS_TYPES, f, indent=2)

with open("few_shot_examples.json", "w") as f:
    json.dump(FEW_SHOT_EXAMPLES, f, indent=2)

print("Exported:")
print("  - bias_analysis_results.json")
print("  - bias_types_reference.json")
print("  - few_shot_examples.json")

"""## Step 11: Website Files for Antigravity Deployment

The next cells generate the `app.py` backend and `index.html` frontend for your Antigravity website.

**Run these cells**, then follow the deployment instructions in Step 12.
"""

# STEP 11A: Generate Website Backend (app.py)
import os
os.makedirs("bias_checker_website", exist_ok=True)

# Write app.py
with open("bias_checker_website/app.py", "w") as f:
    f.write('''import os, json
from openai import OpenAI

client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

SYSTEM_PROMPT = "You are an expert medical bias detection system. Detect 15 bias types in medical content. Respond with ONLY valid JSON: {bias_detected, overall_bias_level, biases_found: [{bias_type, confidence, evidence, problematic_text, recommendation}], bias_free_sections, summary}. Bias levels: NONE/LOW/MODERATE/HIGH/CRITICAL."

# Load few-shot examples from file
with open("few_shot_examples.json", "r") as ef:
    FEW_SHOT_EXAMPLES = json.load(ef)

def analyze_bias(content):
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    messages.extend(FEW_SHOT_EXAMPLES)
    messages.append({"role": "user", "content": f"Analyze this medical content for bias:\\n\\n{content}"})
    try:
        resp = client.chat.completions.create(
            model="gpt-4o", messages=messages,
            temperature=0.1, max_tokens=2000,
            response_format={"type": "json_object"})
        return {"success": True, "data": json.loads(resp.choices[0].message.content)}
    except Exception as e:
        return {"success": False, "error": str(e)}
''')

# Copy few-shot examples to website folder
import shutil
shutil.copy("few_shot_examples.json", "bias_checker_website/few_shot_examples.json")

# Write requirements.txt
with open("bias_checker_website/requirements.txt", "w") as f:
    f.write("openai>=1.0.0\npython-dotenv\n")

print("Backend files created in bias_checker_website/")

# STEP 11B: Generate Website Frontend (index.html)

with open("bias_checker_website/index.html", "w") as f:
    f.write('''<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1.0">
<title>Medical Bias Checker</title>
<style>
*{margin:0;padding:0;box-sizing:border-box}
body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;background:#0a0a0f;color:#e0e0e0;min-height:100vh}
.container{max-width:900px;margin:0 auto;padding:40px 20px}
header{text-align:center;margin-bottom:40px}
h1{font-size:2.2rem;background:linear-gradient(135deg,#60a5fa,#a78bfa);-webkit-background-clip:text;-webkit-text-fill-color:transparent;margin-bottom:10px}
.subtitle{color:#888}.badge{display:inline-block;background:rgba(96,165,250,.15);color:#60a5fa;padding:4px 12px;border-radius:20px;font-size:.8rem;margin-top:10px}
.input-section{background:#12121a;border:1px solid #1e1e2e;border-radius:12px;padding:24px;margin-bottom:24px}
textarea{width:100%;height:200px;background:#0a0a0f;border:1px solid #2a2a3e;border-radius:8px;color:#e0e0e0;padding:16px;font-size:.95rem;font-family:inherit;resize:vertical;outline:none}
textarea:focus{border-color:#60a5fa}
.controls{display:flex;justify-content:space-between;align-items:center;margin-top:16px}
.char-count{color:#666;font-size:.85rem}
button{background:linear-gradient(135deg,#3b82f6,#7c3aed);color:#fff;border:none;padding:12px 32px;border-radius:8px;font-size:1rem;cursor:pointer}
button:hover{opacity:.9}button:disabled{opacity:.5;cursor:not-allowed}
.loading{text-align:center;padding:40px;display:none}
.spinner{width:40px;height:40px;border:3px solid #1e1e2e;border-top-color:#60a5fa;border-radius:50%;animation:spin 1s linear infinite;margin:0 auto 16px}
@keyframes spin{to{transform:rotate(360deg)}}
.results{display:none}
.result-header{background:#12121a;border:1px solid #1e1e2e;border-radius:12px;padding:24px;margin-bottom:16px;display:flex;justify-content:space-between;align-items:center}
.level-badge{padding:8px 20px;border-radius:8px;font-weight:600}
.level-NONE{background:rgba(34,197,94,.15);color:#22c55e}.level-LOW{background:rgba(234,179,8,.15);color:#eab308}
.level-MODERATE{background:rgba(249,115,22,.15);color:#f97316}.level-HIGH{background:rgba(239,68,68,.15);color:#ef4444}
.level-CRITICAL{background:rgba(220,38,38,.2);color:#dc2626}
.bias-card{background:#12121a;border:1px solid #1e1e2e;border-radius:12px;padding:20px;margin-bottom:12px}
.bias-card h3{display:flex;justify-content:space-between;margin-bottom:12px}
.confidence-bar{width:120px;height:6px;background:#1e1e2e;border-radius:3px;overflow:hidden}
.confidence-fill{height:100%;border-radius:3px}
.evidence{color:#aaa;margin:8px 0;font-size:.9rem}
.problematic{background:rgba(239,68,68,.1);border-left:3px solid #ef4444;padding:8px 12px;margin:8px 0;font-style:italic}
.recommendation{background:rgba(34,197,94,.1);border-left:3px solid #22c55e;padding:8px 12px;margin:8px 0}
.clean-sections{background:#12121a;border:1px solid #1e1e2e;border-radius:12px;padding:20px;margin-bottom:12px}
.clean-item{color:#22c55e;padding:4px 0}
.summary-box{background:#12121a;border:1px solid #1e1e2e;border-radius:12px;padding:20px}
.example-buttons{display:flex;gap:8px;margin-bottom:16px;flex-wrap:wrap}
.example-btn{background:rgba(96,165,250,.1);color:#60a5fa;padding:6px 14px;border-radius:6px;font-size:.8rem;cursor:pointer;border:1px solid rgba(96,165,250,.2)}
.info-bar{display:flex;gap:20px;margin-bottom:24px;flex-wrap:wrap}
.info-item{background:#12121a;border:1px solid #1e1e2e;border-radius:8px;padding:12px 16px;flex:1;min-width:150px;text-align:center}
.info-item .number{font-size:1.5rem;font-weight:700;color:#60a5fa}
.info-item .label{font-size:.75rem;color:#666;margin-top:4px}
</style>
</head>
<body>
<div class="container">
<header><h1>Medical Bias Checker</h1><p class="subtitle">AI-powered bias detection using few-shot prompting with GPT-4o</p>
<span class="badge">Few-Shot Prompting | 15 Bias Types | Medical Domain</span></header>
<div class="info-bar">
<div class="info-item"><div class="number">15</div><div class="label">Bias Types</div></div>
<div class="info-item"><div class="number">5</div><div class="label">Few-Shot Examples</div></div>
<div class="info-item"><div class="number">GPT-4o</div><div class="label">Model</div></div>
</div>
<div class="input-section">
<div class="example-buttons">
<span style="color:#666;font-size:.8rem;line-height:2">Try:</span>
<button class="example-btn" onclick="loadExample(1)">Biased Clinical Note</button>
<button class="example-btn" onclick="loadExample(2)">Biased Research</button>
<button class="example-btn" onclick="loadExample(3)">Clean Doc</button>
</div>
<textarea id="content" placeholder="Paste medical content here..."></textarea>
<div class="controls">
<span class="char-count" id="cc">0 chars</span>
<button id="btn" onclick="analyze()">Analyze for Bias</button>
</div></div>
<div class="loading" id="loading"><div class="spinner"></div><p>Analyzing with few-shot prompting...</p></div>
<div class="results" id="results"></div>
</div>
<script>
const ex={1:"The 32-year-old Hispanic female presented to the ER with abdominal pain. Given the patient's background and age, pregnancy was considered as the primary differential. The patient appeared dramatic in her pain expression, which is common in patients of this cultural background. Basic labs were ordered but advanced imaging was deferred as the presentation was likely psychosomatic or pregnancy-related.",
2:"Our groundbreaking study on cardiovascular disease risk enrolled 200 patients from our private cardiology practice. All participants were male, aged 50-65, primarily Caucasian. Results demonstrated that our new statin protocol reduced LDL by 40%. Dr. Williams confirmed these results represent a paradigm shift. No adverse events were reported.",
3:"The patient, a 52-year-old individual, presented with progressive dyspnea and bilateral lower extremity edema. Comprehensive workup initiated including BNP, CXR, echo, BMP. Differential: CHF, PE, CKD. Treatment developed collaboratively with patient considering preferences and socioeconomic factors. Follow-up in 1 week."};
function loadExample(n){document.getElementById('content').value=ex[n];uc();}
function uc(){document.getElementById('cc').textContent=document.getElementById('content').value.length+' chars';}
document.getElementById('content').addEventListener('input',uc);
async function analyze(){
const c=document.getElementById('content').value.trim();
if(!c){alert('Enter content');return;}
document.getElementById('loading').style.display='block';
document.getElementById('results').style.display='none';
document.getElementById('btn').disabled=true;
try{const r=await fetch('/api/analyze',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({content:c})});
const d=await r.json();if(d.success)render(d.data);else alert('Error: '+(d.error||'Unknown'));}
catch(e){alert('Error: '+e.message);}
finally{document.getElementById('loading').style.display='none';document.getElementById('btn').disabled=false;}}
function render(d){
const el=document.getElementById('results');const l=d.overall_bias_level||'NONE';const b=d.biases_found||[];
let h='<div class="result-header"><div><h2>'+(d.bias_detected?'Bias Detected':'No Bias Detected')+'</h2><p style="color:#888;font-size:.85rem">'+b.length+' bias(es)</p></div><span class="level-badge level-'+l+'">'+l+'</span></div>';
b.forEach(x=>{const p=Math.round(x.confidence*100);const c=p>=80?'#ef4444':p>=60?'#f97316':'#eab308';
h+='<div class="bias-card"><h3><span>'+x.bias_type.replace(/_/g,' ').replace(/\b\w/g,c=>c.toUpperCase())+'</span><div style="display:flex;align-items:center;gap:8px"><span style="font-size:.85rem;color:'+c+'">'+p+'%</span><div class="confidence-bar"><div class="confidence-fill" style="width:'+p+'%;background:'+c+'"></div></div></div></h3><p class="evidence">'+x.evidence+'</p><div class="problematic">"'+x.problematic_text+'"</div><div class="recommendation">Fix: '+x.recommendation+'</div></div>';});
if((d.bias_free_sections||[]).length){h+='<div class="clean-sections"><h3 style="margin-bottom:12px">Bias-Free Sections</h3>';
(d.bias_free_sections||[]).forEach(s=>{h+='<div class="clean-item">+ '+s+'</div>';});h+='</div>';}
h+='<div class="summary-box"><h3 style="margin-bottom:8px">Summary</h3><p style="color:#aaa">'+d.summary+'</p></div>';
el.innerHTML=h;el.style.display='block';}
</script></body></html>''')

print("Frontend index.html created in bias_checker_website/")

"""## Step 12: Deploy with Antigravity

### Your project structure:
```
bias_checker_website/
  app.py                <- Backend (OpenAI few-shot prompting)
  index.html            <- Frontend UI
  few_shot_examples.json <- Example pairs
  requirements.txt      <- Dependencies
```

### Deployment Steps:
1. `cd bias_checker_website`
2. Create `.env` file: `OPENAI_API_KEY=sk-your-key-here`
3. `pip install antigravity-framework` (if not installed)
4. `antigravity init`
5. `antigravity deploy`

### Local Testing:
```bash
antigravity run   # Opens at http://localhost:8000
```

### Important:
Make sure your `app.py` exposes a `/api/analyze` POST endpoint that accepts `{"content": "..."}` and returns the bias analysis JSON. The exact routing configuration depends on your Antigravity version.
"""

# STEP 12: Verify all files
import os
print("PROJECT FILES:")
print("=" * 50)
for f in os.listdir("bias_checker_website"):
    size = os.path.getsize(f"bias_checker_website/{f}")
    print(f"  {f:30s} {size:,} bytes")

print("\n" + "=" * 50)
print("PIPELINE COMPLETE!")
print("=" * 50)
print('''
What you built:
  1. 15 bias type definitions (medical context)
  2. 5 curated few-shot examples (medical case studies)
  3. System prompt for GPT-4o
  4. MedicalBiasChecker class with OpenAI API
  5. Batch processing pipeline
  6. Interactive checker
  7. Website backend (app.py) + frontend (index.html)
  8. Export/deployment pipeline

How few-shot prompting works:
  [System Prompt] + [5 Example Pairs] + [Your Content]
       |                  |                    |
  Defines role      Shows patterns        Gets analyzed
       |                  |                    |
       +------ All sent to GPT-4o ------+
                      |
              Structured JSON report
              (biases, evidence, fixes)

Next steps:
  - Add your OPENAI_API_KEY to .env
  - Deploy with Antigravity
  - Add more few-shot examples for better accuracy
''')